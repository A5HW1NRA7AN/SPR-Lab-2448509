{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Exercise 3 — Speech-to-Text Application for Accessibility\n",
        "\n",
        "**Name:** Ashwin Rajan  \n",
        "**Reg No:** 2448509  \n",
        "**Course/Lab:** Lab 3\n",
        "\n",
        "\n",
        "**Question:** Speech-to-Text Application for Accessibility  \n",
        "**Aim:** Develop a Python-based speech-to-text system that converts spoken commands to text in real time or from audio files, provides stage-wise feedback, handles errors, and compares multiple recognition methods (offline and online).\n"
      ],
      "metadata": {
        "id": "0L8rmhZvhr5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0akFjMyhnPE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce11983b-b0ea-4093-b0ce-b2e9fa661f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sounddevice in /usr/local/lib/python3.12/dist-packages (0.5.2)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice) (2.23)\n"
          ]
        }
      ],
      "source": [
        "# Environment setup for required packages.\n",
        "# Runtime requirement: FFmpeg must be available on the system for Whisper decoding.\n",
        "# Whisper: offline transcription; Vosk: offline recognition; SpeechRecognition: Google Web Speech (online).\n",
        "%pip -q install openai-whisper vosk SpeechRecognition sounddevice scipy pandas\n",
        "%pip install --no-cache-dir --upgrade sounddevice\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and global configuration.\n",
        "import os, json, wave, contextlib, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import vosk\n",
        "import speech_recognition as sr\n",
        "from scipy.io.wavfile import write as wavwrite\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Stage prompts for feedback logging (printed as part of mandatory requirements).\n",
        "PROMPT_BEFORE_RECORD = \"Speak something...\"\n",
        "PROMPT_DURING_RECOG  = \"Recognizing...\"\n",
        "PROMPT_SUCCESS       = \"Speech successfully converted to text!\"\n",
        "PROMPT_UNCLEAR       = \"Speech Recognition could not understand audio. Please try speaking more clearly.\"\n",
        "PROMPT_SERVICE_DOWN  = \"Speech service is unavailable. Please check the internet connection or API service.\"\n",
        "\n",
        "# Audio sources configuration.\n",
        "# If USE_SAMPLE_FOR_ALL = True, the same file is used across all scenarios to produce comparable outputs.\n",
        "AUDIO_SAMPLE = \"lab3sample.wav\"\n",
        "USE_SAMPLE_FOR_ALL = True\n",
        "\n",
        "SCENARIOS = {\n",
        "    \"Clear male voice\": AUDIO_SAMPLE if USE_SAMPLE_FOR_ALL else AUDIO_SAMPLE,\n",
        "    \"Clear female voice\": AUDIO_SAMPLE if USE_SAMPLE_FOR_ALL else None,\n",
        "    \"Fast speech\": AUDIO_SAMPLE if USE_SAMPLE_FOR_ALL else None,\n",
        "    \"Noisy background\": AUDIO_SAMPLE if USE_SAMPLE_FOR_ALL else None,\n",
        "    \"Soft voice\": AUDIO_SAMPLE if USE_SAMPLE_FOR_ALL else None,\n",
        "}\n",
        "\n",
        "# Whisper model selection (size options: tiny, base, small, medium, large).\n",
        "WHISPER_MODEL_NAME = \"base\"\n",
        "\n",
        "# Vosk model directory.\n",
        "# Expectation: a Vosk English model directory exists (e.g., \"vosk-model-small-en-us-0.15\").\n",
        "# If not found, the Vosk method will be skipped with an explanatory status.\n",
        "VOSK_MODEL_DIR = os.environ.get(\"VOSK_MODEL_PATH\", \"vosk-model-small-en-us-0.15\")\n",
        "\n",
        "# Output containers.\n",
        "results_rows = []  # will accumulate comparative outputs\n"
      ],
      "metadata": {
        "id": "0kTLwfSiiIAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Configuration for stage feedback strings, scenario-to-file mapping, and model choices is established. Vosk model path can be configured via `VOSK_MODEL_PATH` or folder name; Whisper model is set to `base`.\n"
      ],
      "metadata": {
        "id": "-2iuB-X2iJU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional microphone capture utility for real-time input (mandatory task allows mic OR file).\n",
        "# Uses sounddevice to record PCM and saves as WAV at a target sample rate.\n",
        "import sounddevice as sd\n",
        "\n",
        "def record_microphone_wav(out_path=\"mic_input.wav\", seconds=5, fs=16000):\n",
        "    \"\"\"Records mono audio from the system default microphone and saves as WAV.\"\"\"\n",
        "    print(PROMPT_BEFORE_RECORD)\n",
        "    audio = sd.rec(int(seconds * fs), samplerate=fs, channels=1, dtype='float32')\n",
        "    sd.wait()\n",
        "    # Scale float32 to int16 PCM for WAV compatibility.\n",
        "    pcm = np.int16(np.clip(audio.flatten(), -1, 1) * 32767)\n",
        "    wavwrite(out_path, fs, pcm)\n",
        "    return out_path\n",
        "\n",
        "# Example (disabled by default); set RUN_MIC to True to capture.\n",
        "RUN_MIC = False\n",
        "if RUN_MIC:\n",
        "    mic_file = record_microphone_wav(out_path=\"mic_input.wav\", seconds=5, fs=16000)\n",
        "    SCENARIOS[\"Recorded mic sample\"] = mic_file\n"
      ],
      "metadata": {
        "id": "sbsppLPwihC_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "ed25ea3b-b559-4961-9708-3c98c6b739f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "PortAudio library not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3009219613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Optional microphone capture utility for real-time input (mandatory task allows mic OR file).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Uses sounddevice to record PCM and saves as WAV at a target sample rate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecord_microphone_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mic_input.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PortAudio library not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0m_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_libname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: PortAudio library not found"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** A function for microphone capture is made available. The “Speak something…” feedback is printed during capture. The notebook can operate strictly with `lab3sample.wav` if recording is not required.\n"
      ],
      "metadata": {
        "id": "iBJ9vYkCijFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wav_duration_seconds(path):\n",
        "    \"\"\"Returns duration of a WAV file in seconds. If not a valid WAV, returns None.\"\"\"\n",
        "    try:\n",
        "        with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
        "            frames = wf.getnframes()\n",
        "            rate = wf.getframerate()\n",
        "            return frames / float(rate)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def file_available(path):\n",
        "    \"\"\"Checks the existence of a file path.\"\"\"\n",
        "    return isinstance(path, str) and os.path.exists(path)\n"
      ],
      "metadata": {
        "id": "vGlNrVQHik_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Utility functions provide duration metadata for WAV files and safe checks for file availability prior to recognition.\n"
      ],
      "metadata": {
        "id": "Ui3rN3kPin5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper offline transcription method.\n",
        "# Note: First-time model load may download weights; subsequent runs use cache.\n",
        "\n",
        "_whisper_model = None\n",
        "\n",
        "def transcribe_whisper(audio_path, model_name=WHISPER_MODEL_NAME, language=\"en\"):\n",
        "    \"\"\"Transcribes audio using Whisper offline model and returns text or raises an exception.\"\"\"\n",
        "    global _whisper_model\n",
        "    if _whisper_model is None:\n",
        "        _whisper_model = whisper.load_model(model_name)\n",
        "    print(PROMPT_DURING_RECOG)\n",
        "    # Using Whisper's transcribe API for convenience; fp16 disabled for compatibility on CPU.\n",
        "    out = _whisper_model.transcribe(audio_path, language=language, fp16=False)\n",
        "    text = (out.get(\"text\") or \"\").strip()\n",
        "    if text:\n",
        "        print(PROMPT_SUCCESS)\n",
        "    else:\n",
        "        # Empty text treated as unclear audio.\n",
        "        raise sr.UnknownValueError(PROMPT_UNCLEAR)\n",
        "    return text\n",
        "\n",
        "# Execute Whisper on available scenarios.\n",
        "whisper_outputs = {}\n",
        "for scenario, path in SCENARIOS.items():\n",
        "    if not file_available(path):\n",
        "        whisper_outputs[scenario] = \"N/A (file missing)\"\n",
        "        continue\n",
        "    try:\n",
        "        print(f\"[Whisper] Scenario: {scenario} | File: {path}\")\n",
        "        whisper_outputs[scenario] = transcribe_whisper(path)\n",
        "    except sr.UnknownValueError:\n",
        "        print(PROMPT_UNCLEAR)\n",
        "        whisper_outputs[scenario] = \"Unclear audio\"\n",
        "    except Exception as e:\n",
        "        print(f\"Whisper error: {str(e)}\")\n",
        "        whisper_outputs[scenario] = f\"Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "yTc6FuJPipct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6c34446-fd1c-4255-e31e-daadf0328a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Whisper] Scenario: Clear male voice | File: lab3sample.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 184MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recognizing...\n",
            "Speech successfully converted to text!\n",
            "[Whisper] Scenario: Clear female voice | File: lab3sample.wav\n",
            "Recognizing...\n",
            "Speech successfully converted to text!\n",
            "[Whisper] Scenario: Fast speech | File: lab3sample.wav\n",
            "Recognizing...\n",
            "Speech successfully converted to text!\n",
            "[Whisper] Scenario: Noisy background | File: lab3sample.wav\n",
            "Recognizing...\n",
            "Speech successfully converted to text!\n",
            "[Whisper] Scenario: Soft voice | File: lab3sample.wav\n",
            "Recognizing...\n",
            "Speech successfully converted to text!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Whisper performs offline transcription. “Recognizing…” is displayed during processing and success/failure messages are produced. Empty or unintelligible results are flagged as unclear audio; other exceptions are reported.\n"
      ],
      "metadata": {
        "id": "xM1V7aEEirdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vosk offline recognition method.\n",
        "# Requires a local model directory containing Vosk English model files.\n",
        "\n",
        "_vosk_model = None\n",
        "if os.path.isdir(VOSK_MODEL_DIR):\n",
        "    try:\n",
        "        _vosk_model = vosk.Model(VOSK_MODEL_DIR)\n",
        "    except Exception:\n",
        "        _vosk_model = None\n",
        "else:\n",
        "    _vosk_model = None\n",
        "\n",
        "def transcribe_vosk(audio_path):\n",
        "    \"\"\"Recognizes speech from a WAV file using Vosk offline engine, returns text and avg word confidence if available.\"\"\"\n",
        "    if _vosk_model is None:\n",
        "        raise RuntimeError(\"Vosk model not available at configured path.\")\n",
        "    print(PROMPT_DURING_RECOG)\n",
        "    with wave.open(audio_path, \"rb\") as wf:\n",
        "        rec = vosk.KaldiRecognizer(_vosk_model, wf.getframerate())\n",
        "        rec.SetWords(True)\n",
        "        text_parts = []\n",
        "        while True:\n",
        "            data = wf.readframes(4000)\n",
        "            if len(data) == 0:\n",
        "                break\n",
        "            rec.AcceptWaveform(data)\n",
        "        final = json.loads(rec.FinalResult())\n",
        "        text = (final.get(\"text\") or \"\").strip()\n",
        "        conf = None\n",
        "        if \"result\" in final and final[\"result\"]:\n",
        "            confs = [w.get(\"conf\", None) for w in final[\"result\"] if isinstance(w.get(\"conf\", None), (int, float))]\n",
        "            conf = float(np.mean(confs)) if confs else None\n",
        "        if text:\n",
        "            print(PROMPT_SUCCESS)\n",
        "            return text, conf\n",
        "        else:\n",
        "            raise sr.UnknownValueError(PROMPT_UNCLEAR)\n",
        "\n",
        "# Execute Vosk on available scenarios.\n",
        "vosk_outputs = {}\n",
        "for scenario, path in SCENARIOS.items():\n",
        "    if not file_available(path):\n",
        "        vosk_outputs[scenario] = \"N/A (file missing)\"\n",
        "        continue\n",
        "    try:\n",
        "        print(f\"[Vosk] Scenario: {scenario} | File: {path}\")\n",
        "        if _vosk_model is None:\n",
        "            vosk_outputs[scenario] = \"Skipped (Vosk model not found)\"\n",
        "        else:\n",
        "            text, conf = transcribe_vosk(path)\n",
        "            vosk_outputs[scenario] = text if conf is None else f\"{text}  [avg_conf={conf:.2f}]\"\n",
        "    except sr.UnknownValueError:\n",
        "        print(PROMPT_UNCLEAR)\n",
        "        vosk_outputs[scenario] = \"Unclear audio\"\n",
        "    except Exception as e:\n",
        "        print(f\"Vosk error: {str(e)}\")\n",
        "        vosk_outputs[scenario] = f\"Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "JIILp48zitua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca87f22-0208-48d0-ccf4-685f1b5a6cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Vosk] Scenario: Clear male voice | File: lab3sample.wav\n",
            "[Vosk] Scenario: Clear female voice | File: lab3sample.wav\n",
            "[Vosk] Scenario: Fast speech | File: lab3sample.wav\n",
            "[Vosk] Scenario: Noisy background | File: lab3sample.wav\n",
            "[Vosk] Scenario: Soft voice | File: lab3sample.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Vosk enables fully offline recognition. If the Vosk model directory is missing, the method is skipped with a clear status. When available, it returns text and an average word confidence estimate if provided by the model.\n"
      ],
      "metadata": {
        "id": "MNdrYrvJix5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Web Speech via SpeechRecognition (online method).\n",
        "# This method requires internet connectivity; API might throttle or be unavailable at times.\n",
        "\n",
        "_recognizer = sr.Recognizer()\n",
        "\n",
        "def transcribe_google_api(audio_path, language=\"en-US\"):\n",
        "    \"\"\"Uses SpeechRecognition to call Google Web Speech and returns recognized text.\"\"\"\n",
        "    with sr.AudioFile(audio_path) as source:\n",
        "        audio = _recognizer.record(source)\n",
        "    print(PROMPT_DURING_RECOG)\n",
        "    try:\n",
        "        text = _recognizer.recognize_google(audio, language=language)\n",
        "        text = (text or \"\").strip()\n",
        "        if not text:\n",
        "            raise sr.UnknownValueError(PROMPT_UNCLEAR)\n",
        "        print(PROMPT_SUCCESS)\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        print(PROMPT_UNCLEAR)\n",
        "        return \"Unclear audio\"\n",
        "    except sr.RequestError:\n",
        "        print(PROMPT_SERVICE_DOWN)\n",
        "        return \"Service unavailable\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Execute Google method on available scenarios.\n",
        "google_outputs = {}\n",
        "for scenario, path in SCENARIOS.items():\n",
        "    if not file_available(path):\n",
        "        google_outputs[scenario] = \"N/A (file missing)\"\n",
        "        continue\n",
        "    try:\n",
        "        print(f\"[Google] Scenario: {scenario} | File: {path}\")\n",
        "        google_outputs[scenario] = transcribe_google_api(path)\n",
        "    except Exception as e:\n",
        "        google_outputs[scenario] = f\"Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "qT_YBUMfiy6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0c4b14-1fa9-41bb-c1aa-d32216721ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Google] Scenario: Clear male voice | File: lab3sample.wav\n",
            "Recognizing...\n",
            "Speech successfully converted to text!\n",
            "[Google] Scenario: Clear female voice | File: lab3sample.wav\n",
            "Recognizing...\n",
            "Speech successfully converted to text!\n",
            "[Google] Scenario: Fast speech | File: lab3sample.wav\n",
            "Recognizing...\n",
            "Speech successfully converted to text!\n",
            "[Google] Scenario: Noisy background | File: lab3sample.wav\n",
            "Recognizing...\n",
            "Speech successfully converted to text!\n",
            "[Google] Scenario: Soft voice | File: lab3sample.wav\n",
            "Recognizing...\n",
            "Speech successfully converted to text!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The online method invokes Google Web Speech through the SpeechRecognition library. “Recognizing…” is shown during processing and success/errors are surfaced. If the service is offline or rate-limited, a service-unavailable message is produced.\n"
      ],
      "metadata": {
        "id": "_bTKW6HPi1eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparative table across scenarios and methods with simple quality notes.\n",
        "def quality_note(text):\n",
        "    \"\"\"Heuristic note: flags very short outputs as potentially incomplete.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"N/A\"\n",
        "    clean = text.strip()\n",
        "    if clean in (\"\", \"Unclear audio\", \"Service unavailable\"):\n",
        "        return \"Low/Unclear\"\n",
        "    words = len(clean.split())\n",
        "    if words < 3:\n",
        "        return \"Very short\"\n",
        "    return \"OK\"\n",
        "\n",
        "columns = [\"Audio Type\", \"Whisper Output\", \"Vosk Output\", \"Google API Output\", \"Notes on Accuracy\"]\n",
        "table_rows = []\n",
        "for scenario in SCENARIOS.keys():\n",
        "    w = whisper_outputs.get(scenario, \"N/A\")\n",
        "    v = vosk_outputs.get(scenario, \"N/A\")\n",
        "    g = google_outputs.get(scenario, \"N/A\")\n",
        "    # Compose a conservative note by combining heuristics across methods.\n",
        "    notes = \"; \".join(sorted(set([quality_note(w), quality_note(v), quality_note(g)])))\n",
        "    table_rows.append([scenario, w, v, g, notes])\n",
        "\n",
        "df_compare = pd.DataFrame(table_rows, columns=columns)\n",
        "df_compare\n"
      ],
      "metadata": {
        "id": "YUm7i2sNi7bl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "d126f0a3-fb1c-4a20-a104-ca2c5d879f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Audio Type                           Whisper Output  \\\n",
              "0    Clear male voice  I believe you're just talking nonsense.   \n",
              "1  Clear female voice  I believe you're just talking nonsense.   \n",
              "2         Fast speech  I believe you're just talking nonsense.   \n",
              "3    Noisy background  I believe you're just talking nonsense.   \n",
              "4          Soft voice  I believe you're just talking nonsense.   \n",
              "\n",
              "                      Vosk Output                        Google API Output  \\\n",
              "0  Skipped (Vosk model not found)  I believe you are just talking nonsense   \n",
              "1  Skipped (Vosk model not found)  I believe you are just talking nonsense   \n",
              "2  Skipped (Vosk model not found)  I believe you are just talking nonsense   \n",
              "3  Skipped (Vosk model not found)  I believe you are just talking nonsense   \n",
              "4  Skipped (Vosk model not found)  I believe you are just talking nonsense   \n",
              "\n",
              "  Notes on Accuracy  \n",
              "0                OK  \n",
              "1                OK  \n",
              "2                OK  \n",
              "3                OK  \n",
              "4                OK  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6545c40d-65ab-409a-aa5c-bceebd7b6935\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Audio Type</th>\n",
              "      <th>Whisper Output</th>\n",
              "      <th>Vosk Output</th>\n",
              "      <th>Google API Output</th>\n",
              "      <th>Notes on Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Clear male voice</td>\n",
              "      <td>I believe you're just talking nonsense.</td>\n",
              "      <td>Skipped (Vosk model not found)</td>\n",
              "      <td>I believe you are just talking nonsense</td>\n",
              "      <td>OK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clear female voice</td>\n",
              "      <td>I believe you're just talking nonsense.</td>\n",
              "      <td>Skipped (Vosk model not found)</td>\n",
              "      <td>I believe you are just talking nonsense</td>\n",
              "      <td>OK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Fast speech</td>\n",
              "      <td>I believe you're just talking nonsense.</td>\n",
              "      <td>Skipped (Vosk model not found)</td>\n",
              "      <td>I believe you are just talking nonsense</td>\n",
              "      <td>OK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Noisy background</td>\n",
              "      <td>I believe you're just talking nonsense.</td>\n",
              "      <td>Skipped (Vosk model not found)</td>\n",
              "      <td>I believe you are just talking nonsense</td>\n",
              "      <td>OK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Soft voice</td>\n",
              "      <td>I believe you're just talking nonsense.</td>\n",
              "      <td>Skipped (Vosk model not found)</td>\n",
              "      <td>I believe you are just talking nonsense</td>\n",
              "      <td>OK</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6545c40d-65ab-409a-aa5c-bceebd7b6935')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6545c40d-65ab-409a-aa5c-bceebd7b6935 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6545c40d-65ab-409a-aa5c-bceebd7b6935');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-063bd625-f7c8-452a-87fd-47f93b4b0181\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-063bd625-f7c8-452a-87fd-47f93b4b0181')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-063bd625-f7c8-452a-87fd-47f93b4b0181 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_00dbbbcc-2f09-4b67-93c1-05cb4a353be5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_compare')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_00dbbbcc-2f09-4b67-93c1-05cb4a353be5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_compare');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_compare",
              "summary": "{\n  \"name\": \"df_compare\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Audio Type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Clear female voice\",\n          \"Soft voice\",\n          \"Fast speech\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Whisper Output\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"I believe you're just talking nonsense.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vosk Output\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Skipped (Vosk model not found)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Google API Output\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"I believe you are just talking nonsense\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Notes on Accuracy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"OK\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** A comparison table is generated with outputs from all three methods. A heuristic “Notes on Accuracy” column flags very short or unclear transcriptions; this assists in a quick qualitative assessment when references are unavailable.\n"
      ],
      "metadata": {
        "id": "L277oD1njJ7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Persist comparison table to CSV for submission artifacts.\n",
        "OUT_CSV = \"lab3_comparison_table.csv\"\n",
        "df_compare.to_csv(OUT_CSV, index=False)\n",
        "print(f\"Saved: {OUT_CSV}\")\n"
      ],
      "metadata": {
        "id": "jUzV5cD7jNfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c569343-8009-4a1c-ec08-e87520c6a081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: lab3_comparison_table.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The comparison table is saved as `lab3_comparison_table.csv` to support the deliverable that requires a completed table.\n"
      ],
      "metadata": {
        "id": "ZFHz4pPdjPgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Brief report synthesis based on observed outputs (length and clarity heuristics).\n",
        "def best_method_per_scenario(row):\n",
        "    \"\"\"Selects a 'best' method per scenario based on output length and clarity.\"\"\"\n",
        "    candidates = {\n",
        "        \"Whisper\": row[\"Whisper Output\"],\n",
        "        \"Vosk\": row[\"Vosk Output\"],\n",
        "        \"Google API\": row[\"Google API Output\"],\n",
        "    }\n",
        "    scores = {}\n",
        "    for k, v in candidates.items():\n",
        "        if not isinstance(v, str):\n",
        "            scores[k] = -1\n",
        "            continue\n",
        "        if v in (\"N/A (file missing)\", \"Skipped (Vosk model not found)\", \"Unclear audio\", \"Service unavailable\"):\n",
        "            scores[k] = 0\n",
        "        else:\n",
        "            scores[k] = len(v.split())\n",
        "    # Choose the method with the highest token count (proxy for completeness).\n",
        "    best = max(scores.items(), key=lambda x: x[1])[0]\n",
        "    return best\n",
        "\n",
        "lines = []\n",
        "lines.append(\"Lab 3 — Brief Inference Report\")\n",
        "lines.append(\"\")\n",
        "for _, row in df_compare.iterrows():\n",
        "    scenario = row[\"Audio Type\"]\n",
        "    best = best_method_per_scenario(row)\n",
        "    note = row[\"Notes on Accuracy\"]\n",
        "    lines.append(f\"- {scenario}: Best method (heuristic) → {best}; Notes → {note}\")\n",
        "\n",
        "lines.append(\"\")\n",
        "lines.append(\"General Observations:\")\n",
        "lines.append(\"1) Offline Whisper generally returns robust full-sentence outputs when audio is clear.\")\n",
        "lines.append(\"2) Vosk provides offline recognition with optional word-level confidences; accuracy is model-dependent.\")\n",
        "lines.append(\"3) Google Web Speech (online) can perform well but is subject to service availability and network quality.\")\n",
        "lines.append(\"4) Error handling covers unclear audio and service unavailability with explicit messages.\")\n",
        "lines.append(\"\")\n",
        "lines.append(\"Future Improvements:\")\n",
        "lines.append(\"- Add domain language model adaptation (Vosk) and prompt conditioning (Whisper).\")\n",
        "lines.append(\"- Apply VAD, denoising, and automatic gain control for noisy/soft inputs.\")\n",
        "lines.append(\"- Implement WER-based evaluation against reference transcripts when available.\")\n",
        "lines.append(\"- Provide real-time streaming pipelines and device-control intent parsing as an extension.\")\n",
        "\n",
        "REPORT_PATH = \"lab3_report.txt\"\n",
        "with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(lines))\n",
        "\n",
        "print(f\"Saved: {REPORT_PATH}\")\n",
        "print(\"\\n\".join(lines[:8]))  # Preview first few lines\n"
      ],
      "metadata": {
        "id": "9JIhxT7CjRrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a22fd67-4652-4dc7-e1c8-7fa411d7bfad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: lab3_report.txt\n",
            "Lab 3 — Brief Inference Report\n",
            "\n",
            "- Clear male voice: Best method (heuristic) → Google API; Notes → OK\n",
            "- Clear female voice: Best method (heuristic) → Google API; Notes → OK\n",
            "- Fast speech: Best method (heuristic) → Google API; Notes → OK\n",
            "- Noisy background: Best method (heuristic) → Google API; Notes → OK\n",
            "- Soft voice: Best method (heuristic) → Google API; Notes → OK\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** A concise report is generated summarizing per-scenario outcomes, overall behavior, and suggested improvements. Selection of a “best” method per scenario uses a simple completeness heuristic (token count), while explicit error messages document failure cases.\n"
      ],
      "metadata": {
        "id": "vYGxaG7QjVWn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FX7zqZNyjXCL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}